{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Vasenkov/vas_env/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/home/user/Vasenkov/vas_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "# torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model = whisper.load_model('small').to(device)\n",
    "# model = whisper.load_model('small').to(device)\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = 'True'\n",
    "import pandas as pd\n",
    "# from datasets import Dataset, load_metric\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Trainer, TrainingArguments\n",
    "import soundfile as sf\n",
    "# processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\", language=\"ru\")\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "# model = whisper.load_model('medium')\n",
    "from whisper import DecodingResult\n",
    "from whisper.decoding import DecodingOptions\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\n",
    "        model.is_multilingual,\n",
    "        num_languages=model.num_languages,\n",
    "        language='ru',\n",
    "        task='transcribe',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_embeding2 import processor\n",
    "\n",
    "phrases = ['Машинист поезда 7 на приближении к станции Бабынино.', 'Привет, я машинист локомотива номер 131 на приближении к станции Москва, как дела?.', 'Да, аллё, машинист состава 35 на подходе к станции Калуга-1, что у вас случилось?']\n",
    "for phrase in phrases:\n",
    "    processor.run(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_csv(filename, data):\n",
    "    with open(filename, mode='a+', newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for array, label in data:\n",
    "            # Преобразование массива в строку JSON\n",
    "            array_str = json.dumps(array.tolist())\n",
    "            writer.writerow([array_str, str(label)])\n",
    "\n",
    "def read_data_from_csv(filename):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, mode='r', encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Пропускаем первую строку с заголовком\n",
    "        \n",
    "        for row in reader:\n",
    "            try:\n",
    "                embedding_str = row[0]\n",
    "                label = int(row[1])\n",
    "                # Преобразование строки в список чисел\n",
    "                embedding_json = json.loads(embedding_str)\n",
    "                embedding_array = np.array(embedding_json)\n",
    "                t = torch.from_numpy(embedding_array)\n",
    "                embeddings.append(t)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке строки: {row}. Причина: {str(e)}\")\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(file_):\n",
    "\n",
    "   audio = whisper.load_audio(file_)\n",
    "   tensor = whisper.log_mel_spectrogram(audio)\n",
    "   trim_tensor = whisper.pad_or_trim(tensor,length=3000)\n",
    "   print(trim_tensor.shape)\n",
    "   segment = trim_tensor.to(device)\n",
    "   print(type(segment), segment.shape, segment.device)\n",
    "   #rez = decode_with_fallback(trim_tensor, )\n",
    "\n",
    "   options = DecodingOptions(temperature=0)\n",
    "   decode_result = model.decode(segment.to(device), options)\n",
    "   embedding = decode_result.embedding\n",
    "   #trim_tensor = whisper.pad_or_trim(embedding,length=3000).unsqueeze(0)\n",
    "   return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "res = get_embedding('dataset/0/record_out.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "res1, res2 = get_embedding('dataset/0/record_out.wav'), get_embedding('dataset/0/record_out1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9937], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(res1.squeeze(-2), res2.squeeze(-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формирование эмбеддингов по всем примерам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 8\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 8\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 8\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 5\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 5\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 5\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 5\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 9\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 9\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 9\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 4\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 4\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 4\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 4\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 3\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 3\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 3\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 3\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 7\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 7\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 7\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 1\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 1\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 1\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 1\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 1\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 1\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 1\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 0\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 0\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 0\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 0\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 0\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 0\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 2\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 2\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 2\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 2\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 2\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 2\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 6\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 6\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 6\n",
      "torch.Size([80, 3000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000]) cuda:0\n",
      "(1, 1, 768) 6\n"
     ]
    }
   ],
   "source": [
    "dir_ = 'dataset'\n",
    "filename = 'embeddings.csv'\n",
    "\n",
    "if not os.path.exists(filename):  # Если файл не существует, создаём его\n",
    "    with open(filename, mode='w', newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['embedding', 'label'])  # Добавляем заголовок\n",
    "\n",
    "for root, dirs, files in os.walk(dir_, topdown=False):\n",
    "    for name in files:\n",
    "        if name.endswith('.ipynb'):  # Пропускаем файлы Jupyter Notebook\n",
    "            continue\n",
    "        file_path = os.path.join(root, name)\n",
    "        \n",
    "        emb = get_embedding(file_path)\n",
    "        emb = emb.tolist()\n",
    "        emb = np.array(emb)\n",
    "        \n",
    "        label = int(file_path.split('/')[-2])\n",
    "        \n",
    "        data_list = [(emb, label)]\n",
    "        print(emb.shape, label)\n",
    "        write_data_to_csv(filename, data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение эмбеддингов по косинусному сходству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "cos_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, labels = read_data_from_csv('embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    for j in range(i + 1, len(labels)):  # Начинаем со следующего элемента после текущего\n",
    "        if labels[i] == labels[j]:\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            new_labels.append(0)\n",
    "         \n",
    "        similarity = torch.nn.functional.cosine_similarity(embeddings[i].squeeze(-2), embeddings[j].squeeze(-2))\n",
    "        cos_list.append(similarity.item())\n",
    "        # print(similarity.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index0 = np.where(np.array(new_labels) == 0)\n",
    "index1 = np.where(np.array(new_labels) == 1)\n",
    "\n",
    "cos_list = np.array(cos_list)\n",
    "\n",
    "plt.hist(cos_list[index0])\n",
    "plt.hist(cos_list[index1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_dictor_my/7/record_out(7).wav\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "rate must be specified when data is a numpy array or list of audio samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m audio_file_path \u001b[38;5;241m=\u001b[39m audio_file_path[\u001b[38;5;241m9\u001b[39m:]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(audio_file_path)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mipd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Vasenkov/vas_env/lib/python3.9/site-packages/IPython/lib/display.py:129\u001b[0m, in \u001b[0;36mAudio.__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate must be specified when data is a numpy array or list of audio samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m Audio\u001b[38;5;241m.\u001b[39m_make_wav(data, rate, normalize)\n",
      "\u001b[0;31mValueError\u001b[0m: rate must be specified when data is a numpy array or list of audio samples."
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "audio_file_path = 'Vasenkov/one_dictor_my/7/record_out(7).wav'\n",
    "audio_file_path = audio_file_path[9:]\n",
    "print(audio_file_path)\n",
    "ipd.Audio(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    for j in lst:\n",
    "        emb1 = torch.FloatTensor(i.tokens)\n",
    "        emb2 = torch.FloatTensor(j.tokens)\n",
    "        trim_tensor1 = whisper.pad_or_trim(emb1,length=3000)\n",
    "        trim_tensor2 = whisper.pad_or_trim(emb2,length=3000)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0364e+04, 8.5670e+03, 3.6050e+03, 1.0442e+04, 4.2510e+03, 1.0250e+03,\n",
       "        4.4477e+04, 8.0500e+02, 2.4498e+04, 6.9800e+02, 2.4051e+04, 4.4330e+03,\n",
       "        2.2090e+03, 2.8010e+03, 2.6510e+04, 5.6800e+02, 3.8427e+04, 8.2540e+03,\n",
       "        5.0649e+04, 5.0649e+04, 7.7600e+02, 3.6847e+04, 1.1350e+03, 8.0500e+02,\n",
       "        1.3000e+01, 5.0712e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_emb[0][0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = nn.Embedding(51000,  128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3000, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = E(t.int())\n",
    "et.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2990])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = nn.Conv1d(128, 4, 11)\n",
    "ct = C(torch.tensor(et.detach().numpy().transpose( [0, 2, 1])))\n",
    "ct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = nn.Conv1d(3000, 4, 5, stride=1, padding=2)\n",
    "ct = C(et)\n",
    "ct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2 = nn.Conv1d(4, 1, 5, stride=1, padding=2)\n",
    "ct2 = C2(ct)\n",
    "ct2.shape\n",
    "\n",
    "#размерность x на выходе 1 1 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbDataset(data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path        \n",
    "        self.embeddings, self.labels = read_data_from_csv(self.path)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # !!! СДЕЛАТЬ ЧТОБЫ БРАЛОСЬ ИТЕРАТИВНО, А НЕ ВСЁ СРАЗУ\n",
    "        # embeddings, labels = read_data_from_csv(self.path)\n",
    "        print(item)\n",
    "        label = self.labels[item]\n",
    "        label = torch.tensor(label) \n",
    "        t = self.embeddings[item]\n",
    "        \n",
    "        # E = nn.Embedding(51000,  128)\n",
    "        # print(t)\n",
    "        # et = E(t.int())\n",
    "\n",
    "        # C = nn.Conv1d(3000, 4, 5, stride=1, padding=2)\n",
    "        # ct = C(et)\n",
    "        # C2 = nn.Conv1d(4, 1, 5, stride=1, padding=2)\n",
    "        # ct2 = C2(ct)\n",
    "        \n",
    "        # label = torch.tensor(self.labels[item])        \n",
    "        return t, label\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "\n",
    "d_train = EmbDataset(\"embeddings.csv\")\n",
    "train_data = data.DataLoader(d_train, batch_size=6, shuffle=True)\n",
    "\n",
    "# it = iter(train_data)\n",
    "# x, y = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Триплеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "39\n",
      "23\n",
      "1\n",
      "22\n",
      "42\n",
      "torch.Size([6, 1, 1, 768])\n",
      "Epoch 1/100, Loss: 0.0445\n",
      "8\n",
      "27\n",
      "36\n",
      "32\n",
      "34\n",
      "37\n",
      "torch.Size([6, 1, 1, 768])\n",
      "Epoch 2/100, Loss: 0.0487\n",
      "43\n",
      "14\n",
      "41\n",
      "11\n",
      "10\n",
      "33\n",
      "torch.Size([6, 1, 1, 768])\n",
      "Epoch 3/100, Loss: 0.0481\n",
      "6\n",
      "35\n",
      "24\n",
      "9\n",
      "20\n",
      "21\n",
      "torch.Size([6, 1, 1, 768])\n",
      "Epoch 4/100, Loss: 0.0508\n",
      "40\n",
      "5\n",
      "3\n",
      "18\n",
      "26\n",
      "30\n",
      "torch.Size([6, 1, 1, 768])\n",
      "Epoch 5/100, Loss: 0.0466\n",
      "19\n",
      "17\n",
      "38\n",
      "4\n",
      "28\n",
      "29\n",
      "torch.Size([6, 1, 1, 768])\n",
      "Epoch 6/100, Loss: 0.0473\n",
      "15\n",
      "31\n",
      "16\n",
      "13\n",
      "25\n",
      "2\n",
      "torch.Size([6, 1, 1, 768])\n",
      "Epoch 7/100, Loss: 0.0485\n",
      "7\n",
      "12\n",
      "torch.Size([2, 1, 1, 768])\n",
      "Epoch 8/100, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from pytorch_metric_learning import losses\n",
    "from torch import nn\n",
    "from random import randint\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.miners import TripletMarginMiner\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # self.embedding = nn.Embedding(input_dim, 128)\n",
    "        # self.conv = nn.Conv1d()\n",
    "        self.linear = nn.Linear(input_dim, 128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "\n",
    "cosine_distance = CosineSimilarity()\n",
    "model = LinearClassifier(768)\n",
    "loss_func = losses.TripletMarginLoss(distance=cosine_distance)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i, (data, label) in enumerate(train_data):\n",
    "    data = data.to(dtype=torch.float32)\n",
    "    print(data.shape)\n",
    "    optimizer.zero_grad()\n",
    "    # print(data.dtype)\n",
    "    embeddings = model(data).squeeze()\n",
    "    # print(embeddings.shape, label)\n",
    "    loss = loss_func(embeddings, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {i+1}/{100}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     k = randint(0, total-1)\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X)\n",
    "#     loss = criterion(outputs.squeeze(), y.float())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}/{100}, Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
